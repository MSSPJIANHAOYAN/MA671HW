---
title: "Smoothing homework"
author: "Gunangyan"
date: "2/28/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(boot)
library(splines)
```

###Q3 Suppose we fit a curve with basis functions b1(X)=X, b2(X)=(X − 1)2I(X ≥ 1). (Note that I(X ≥ 1) equals 1 for X ≥ 1and0 otherwise.) We fit the linear regression model

```{r}
x = -2:2
y = 1+x+-2*(x-1)^2*I(x>1)
plot(x,y)
```

###The curve is linear between −2 and 1: y=1+x and quadratic between 1 and 2: y=1+x−2(x−1)2


###Q9
```{r}
library(MASS)
set.seed(1)
fit1 <- lm(nox~poly(dis,3),data = Boston)
summary(fit1)

dislims<-range(Boston$dis)
dis.grid <- seq(from = dislims[1], to = dislims[2], by = 0.1)
preds <- predict(fit1, list(dis = dis.grid))
plot(nox~dis,data=Boston)
lines(x=dis.grid,y=preds)
```

```{r}
rss<-rep(NA,10)
for (i in 1:10) {
  fit<-lm(nox~poly(dis,i),data = Boston)
  rss[i]<-sum(fit$residuals^2)
  
}
plot(x=1:10,y=rss)
```
The residuals sum of squares decreases.

```{r}
deltas <- rep(NA, 10)
for (i in 1:10) {
    fit <- glm(nox ~ poly(dis, i), data = Boston)
    deltas[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
plot(1:10, deltas, xlab = "Degree", ylab = "Test MSE", type = "l")
```
###The degree 4 minimizes MSE.

###D
```{r}
fit <- lm(nox ~ bs(dis, knots = c(4, 7, 11)), data = Boston)
summary(fit)
```

```{r}
pred <- predict(fit, list(dis = dis.grid))
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, preds, col = "red", lwd = 2)
```

###E
```{r}
rss <- rep(NA, 16)
for (i in 3:16) {
    fit <- lm(nox ~ bs(dis, df = i), data = Boston)
    rss[i] <- sum(fit$residuals^2)
}
plot(3:16, rss[-c(1, 2)], xlab = "Degrees of freedom", ylab = "RSS", type = "l")
```
###It will decrease when freedom reaches to 14 and the increase slightly.


###f
```{r,warning=FALSE}
cv <- rep(NA, 16)
for (i in 3:16) {
    fit <- glm(nox ~ bs(dis, df = i), data = Boston)
    cv[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
plot(3:16, cv[-c(1, 2)], xlab = "Degrees of freedom", ylab = "Test MSE", type = "l")
```

###Q10

###A
```{r,warning=FALSE}
library(leaps)
#set.seed(1)
#save(College_test,file = "College_test.RData")
#save(College_train,file = "College_train.Rdata")
#train <- sample(length(Outstate), length(Outstate) / 2)
#test <- -train
load("College_train.RData")
load("College_test.RData")
fit <- regsubsets(Outstate ~ ., data = College_train, nvmax = 17, method = "forward")
fit.summary <- summary(fit)
par(mfrow = c(1, 3))
plot(fit.summary$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
min.cp <- min(fit.summary$cp)
std.cp <- sd(fit.summary$cp)
abline(h = min.cp + 0.2 * std.cp, col = "red", lty = 2)
abline(h = min.cp - 0.2 * std.cp, col = "red", lty = 2)
plot(fit.summary$bic, xlab = "Number of variables", ylab = "BIC", type='l')
min.bic <- min(fit.summary$bic)
std.bic <- sd(fit.summary$bic)
abline(h = min.bic + 0.2 * std.bic, col = "red", lty = 2)
abline(h = min.bic - 0.2 * std.bic, col = "red", lty = 2)
plot(fit.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "l", ylim = c(0.4, 0.84))
max.adjr2 <- max(fit.summary$adjr2)
std.adjr2 <- sd(fit.summary$adjr2)
abline(h = max.adjr2 + 0.2 * std.adjr2, col = "red", lty = 2)
abline(h = max.adjr2 - 0.2 * std.adjr2, col = "red", lty = 2)
fit <- regsubsets(Outstate ~ ., data = College_test, method = "forward")
coeffs <- coef(fit, id = 6)
names(coeffs)
```

###B
```{r}
library(gam)
fit <- gam(Outstate ~ Private + s(Room.Board, df = 2) + s(PhD, df = 2) + 
             s(perc.alumni, df = 2) + s(Expend, df = 5) + s(Grad.Rate, df = 2), data=College_train)
par(mfrow = c(2, 3))
plot(fit, se = T, col = "blue")
```

###c
```{r}
preds <- predict(fit, College_test)
err <- mean((College_test$Outstate - preds)^2)
err
tss <- mean((College_test$Outstate - mean(College_test$Outstate))^2)
rss <- 1 - err / tss
rss
```

###d
```{r}
summary(fit)
```
###ANOVA shows a strong evidence of non-linear relationship between “Outstate” and “Expend”“, and a moderately strong non-linear relationship (using p-value of 0.05) between”Outstate" and “Grad.Rate”" or “PhD”.

###11

###a&b
```{r}
set.seed(1)
y <- rnorm(100)
x1 <- rnorm(100)
x2 <- rnorm(100)
beta1 <- 3
```

###c
```{r}
a <- y - beta1 * x1
beta2 <- lm(a ~ x2)$coef[2]
```

###d
```{r}
a <- y - beta2 * x2
beta1 <- lm(a ~ x1)$coef[2]
```

###e
```{r}
iter <- 10
df <- data.frame(0.0, 0.27, 0.0)
names(df) <- c('beta0', 'beta1', 'beta2')
for (i in 1:iter) {
  beta1 <- df[nrow(df), 2]
  a <- y - beta1 * x1
  beta2 <- lm(a ~ x2)$coef[2]
  a <- y - beta2 * x2
  beta1 <- lm(a ~ x1)$coef[2]
  beta0 <- lm(a ~ x1)$coef[1]
  print(beta0)
  print(beta1)
  print(beta2)
  df[nrow(df) + 1,] <- list(beta0, beta1, beta2)
}
```


```{r}
plot(df$beta0, col = 'red', type = 'l')
lines(df$beta1, col = 'blue')
lines(df$beta2, col = 'green')
```

###f
```{r}
plot(df$beta0, col = 'red', type = 'l')
lines(df$beta1, col = 'blue')

lines(df$beta2, col = 'green')
res <- coef(lm(y ~ x1 + x2))
abline(h = res[1], col = 'darkred', lty = 2)

abline(h = res[2], col = 'darkblue', lty = 2)
abline(h = res[3], col = 'darkgreen', lty = 2)
```


